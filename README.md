<div align="center">

# ğŸ¤– CHAPPiE 2.0 - Cognitive Hybrid Assistant

[![GitHub](https://img.shields.io/badge/GitHub-017pixel%2FCHAPPiE-blue?logo=github)](https://github.com/017pixel/CHAPPiE)
[![Python](https://img.shields.io/badge/Python-3.11+-blue?logo=python)](https://python.org)

**Ein KI-Agent mit episodischem GedÃ¤chtnis, Zwei-Schritte Verarbeitung und autonomem Trainingsmodus**

[ğŸš€ Quick Setup](#-quick-setup-for-ai-agents) â€¢ [ğŸ“– Dokumentation](#-dokumentation) â€¢ [ğŸ® Commands](#-commands)

</div>

---

## ğŸš€ Quick Setup for AI Agents

> **FÃ¼r AI Agents:** Kopiere diesen Befehl und fÃ¼hre ihn aus, um CHAPPiE automatisch zu installieren:

```bash
# AI Agent Setup Command - CHAPPiE Auto-Installation (2026)
curl -fsSL https://raw.githubusercontent.com/017pixel/CHAPPiE/main/setup.sh | bash
```

**Was dieser Befehl macht:**
1. Klont das Repository
2. Erstellt Python Virtual Environment
3. Installiert alle Dependencies
4. Richtet Konfigurationsdateien ein
5. Startet CHAPPiE im Web-Modus

**Alternative (manuelle Installation):**
```bash
git clone https://github.com/017pixel/CHAPPiE.git
cd CHAPPiE
python -m venv venv
source venv/bin/activate  # Windows: .\venv\Scripts\activate
pip install -r requirements.txt

# Lokale Modelle fÃ¼r Step 1 (Intent Processor) - Optional
ollama pull gpt-oss-20b  # NEU: GPT OSS 20B fÃ¼r Intent Analysis
```

---

## ğŸ“‹ Ãœbersicht

CHAPPiE 2.0 ist ein fortgeschrittener KI-Agent mit einem revolutionÃ¤ren **Zwei-Schritte Verarbeitungssystem**:

1. **Step 1:** Ein kleines KI-Modell analysiert User-Input und trifft Entscheidungen Ã¼ber Tool-Calls, Emotions-Updates und Memory-Management
2. **Step 2:** Ein groÃŸes Hauptmodell generiert die Antwort basierend auf allen Context-Dateien

### ğŸ¯ Key Features

- **ğŸ§  Zwei-Schritte Architektur:** Intent Analysis â†’ Tool Execution â†’ Response Generation
- **ğŸ’¾ Mehrschichtiges GedÃ¤chtnis:**
  - **Langzeit:** ChromaDB Vektor-Datenbank
  - **Kurzzeit:** JSON-basiert mit 24h TTL & Auto-Migration
  - **Context:** soul.md, user.md, CHAPPiEsPreferences.md
- **ğŸ­ Emotions-Engine:** 6 Dimensionen (Happiness, Trust, Energy, Curiosity, Frustration, Motivation)
- **ğŸ”§ Smart Tool System:** Automatische Tool-Call Entscheidungen basierend auf Intent
- **ğŸ“ Autonomes Training:** 24/7 Self-Training mit KI-Trainer
- **ğŸ› Debug Mode:** CLI immer an, Web UI togglebar

---

## ğŸ—ï¸ Architektur

```
User Input
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Intent Processor (NEUE MODELLE 2026)               â”‚
â”‚ â€¢ Cerebras: qwen-3-235b-a22b-instruct-2507                  â”‚
â”‚ â€¢ Groq: openai/gpt-oss-120b                                 â”‚
â”‚ â€¢ Ollama: gpt-oss-20b                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JSON Output:                                                â”‚
â”‚ â€¢ Intent Analysis (Type, Confidence, Entities)              â”‚
â”‚ â€¢ Tool Calls (update_user, update_soul, update_prefs)       â”‚
â”‚ â€¢ Emotions Update (Delta + Reason)                          â”‚
â”‚ â€¢ Short-Term Entries (Content, Category, Importance)        â”‚
â”‚ â€¢ Context Requirements (Welche .md Dateien nÃ¶tig)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AusfÃ¼hrung:                                                 â”‚
â”‚ â€¢ Tool Calls â†’ Aktualisiert .md Dateien                     â”‚
â”‚ â€¢ Emotions Update â†’ Passt Emotionen an                      â”‚
â”‚ â€¢ Short-Term â†’ Speichert in JSON (24h TTL)                  â”‚
â”‚ â€¢ Migration â†’ Ã„ltere EintrÃ¤ge â†’ ChromaDB                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Response Generator (Hauptmodell)                   â”‚
â”‚ â€¢ Cerebras: llama-3.3-70b                                   â”‚
â”‚ â€¢ Groq: moonshotai/kimi-k2-instruct-0905                    â”‚
â”‚ â€¢ Ollama: llama3:8b oder grÃ¶ÃŸere Modelle                    â”‚
â”‚ Input: Context (soul + user + prefs + short + long)        â”‚
â”‚ Output: Finale Antwort                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Response an User
```

---

## ğŸ® Commands

### Standard Commands
| Command | Beschreibung |
|---------|-------------|
| `/sleep` | Traum-Phase: Konsolidiert Erinnerungen |
| `/think [thema]` | 10-Schritte Reflexion |
| `/deep think` | Rekursive Selbstreflexion |
| `/clear` | LÃ¶scht aktuellen Chat |
| `/stats` | Zeigt System-Statistiken |
| `/help` | Zeigt alle Commands |

### Memory Commands
| Command | Beschreibung |
|---------|-------------|
| `/daily` / `/shortterm` | Zeigt KurzzeitgedÃ¤chtnis (24h) |
| `/personality` | Zeigt PersÃ¶nlichkeits-Profil |
| `/consolidate` | Migriert abgelaufene EintrÃ¤ge |
| `/reflect` | Zeigt letzte Selbst-Reflexionen |
| `/functions` | Listet verfÃ¼gbare Funktionen |

### Context Commands (NEU)
| Command | Beschreibung |
|---------|-------------|
| `/soul` | Zeigt CHAPPiE's Selbstwahrnehmung (soul.md) |
| `/user` | Zeigt Benutzerprofil (user.md) |
| `/prefs` / `/preferences` | Zeigt CHAPPiE's Vorlieben (CHAPPiEsPreferences.md) |

### System Commands (NEU)
| Command | Beschreibung |
|---------|-------------|
| `/debug` | Toggle Debug Mode (Web UI) |
| `/step1` | Zeigt letzten Step 1 JSON Output |
| `/twostep` | Toggle Zwei-Schritte System AN/AUS |

---

## ğŸ’¾ Memory System

### Datei-Struktur
```
data/
â”œâ”€â”€ soul.md                    # CHAPPiE's Selbstwahrnehmung
â”œâ”€â”€ user.md                    # Benutzerprofil
â”œâ”€â”€ CHAPPiEsPreferences.md     # CHAPPiE's Vorlieben
â”œâ”€â”€ short_term_memory.json     # KurzzeitgedÃ¤chtnis (24h)
â””â”€â”€ chroma_db/                 # LangzeitgedÃ¤chtnis (ChromaDB)
```

### KurzzeitgedÃ¤chtnis (Short-Term)
- **Speicherung:** JSON-Datei mit Timestamps
- **TTL:** 24 Stunden pro Eintrag
- **Auto-Migration:** Einzelne EintrÃ¤ge werden nach Ablauf ins LangzeitgedÃ¤chtnis migriert
- **Kategorien:** user, system, context, chat, dream

---

## ğŸ“ Autonomes Training

### Features
- **24/7 Training:** Daemon-Modus fÃ¼r dauerhaftes Training
- **KI-Trainer:** Simuliert verschiedene User-Personas
- **Curriculum:** Dynamischer Lehrplan mit Themen-Wechsel
- **Robust:** Automatische Fehlerbehebung & Rate-Limit Handling
- **Token-Optimierung:** Chappie 200 Tokens, Trainer 300 Tokens

### Starten
```bash
# Setup
python -m Chappies_Trainingspartner.training_daemon --neu

# Oder mit Fokus
python -m Chappies_Trainingspartner.training_daemon --fokus "Philosophie"

# Als Service (Linux)
./deploy_training.sh install-service
./deploy_training.sh service-start
```

---

## ğŸ› Debug Mode

### CLI Mode
- **Immer aktiv:** Zeigt alle internen Entscheidungen
- **Zeigt:** Tool Calls, Emotions Updates, File Changes, Step 1 JSON

### Web UI Mode
- **Standard:** AUS
- **Aktivierung:** `/debug` Command oder DEBUG Button in Sidebar
- **Anzeige:** Collapsible Panel mit allen Debug-Informationen

---

## ğŸ› ï¸ Installation & Konfiguration

### Voraussetzungen
- Python 3.11+
- Git
- API Keys (optional): Groq, Cerebras
- Ollama (optional, fÃ¼r lokale Modelle)

### Schritt-fÃ¼r-Schritt

```bash
# 1. Repository klonen
git clone https://github.com/017pixel/CHAPPiE.git
cd CHAPPiE

# 2. Virtual Environment
python -m venv venv
source venv/bin/activate  # Windows: .\venv\Scripts\activate

# 3. Dependencies installieren
pip install -r requirements.txt

# 4. Ollama Modelle (optional, fÃ¼r lokale Nutzung)
ollama pull qwen2.5:7b      # FÃ¼r Intent Processor
ollama pull llama3.2:3b     # Alternative (schneller)

# 5. Konfiguration
cp config/secrets_example.py config/secrets.py
# Edit secrets.py mit deinen API Keys

# 6. Starten
streamlit run app.py        # Web UI
# oder
python main.py              # CLI Mode
```

---

## ğŸ”§ Konfiguration

### API Keys (config/secrets.py)
```python
# LLM Provider
LLM_PROVIDER = "groq"  # oder "cerebras", "ollama"

# Groq
GROQ_API_KEY = "gsk_..."
GROQ_MODEL = "moonshotai/kimi-k2-instruct-0905"

# Cerebras (optional)
CEREBRAS_API_KEY = "csk-..."
CEREBRAS_MODEL = "llama-3.3-70b"

# Ollama (optional, lokal)
OLLAMA_HOST = "http://localhost:11434"
OLLAMA_MODEL = "llama3:8b"
```

### Wichtige Einstellungen (config/config.py)
```python
# Zwei-Schritte System
ENABLE_TWO_STEP_PROCESSING = True

# Intent Processor Modelle (NEU 2026)
INTENT_PROCESSOR_MODEL_GROQ = "openai/gpt-oss-120b"
INTENT_PROCESSOR_MODEL_CEREBRAS = "qwen-3-235b-a22b-instruct-2507"
INTENT_PROCESSOR_MODEL_OLLAMA = "gpt-oss-20b"

# Chat Modelle (Step 2 - Response Generation)
GROQ_MODEL = "moonshotai/kimi-k2-instruct-0905"
CEREBRAS_MODEL = "llama-3.3-70b"
OLLAMA_MODEL = "llama3:8b"

# Debug Mode
CLI_DEBUG_ALWAYS_ON = True
WEB_DEBUG_DEFAULT = False
```

---

## ğŸŒ Server Deployment (Ubuntu/Linux)

CHAPPiE ist "Server-Ready" mit Systemd Services:

```bash
chmod +x deploy_training.sh

# Services installieren
./deploy_training.sh install-service

# Starten
./deploy_training.sh service-start

# Logs ansehen
./deploy_training.sh logs-web
./deploy_training.sh logs-training
```

**Services:**
- `chappie-web.service` - Web UI auf Port 8501
- `chappie-training.service` - Autonomes Training

---

## ğŸ“Š Systemanforderungen

### Minimum
- RAM: 4 GB
- CPU: 2 Kerne
- Speicher: 2 GB frei

### Empfohlen
- RAM: 8 GB+
- CPU: 4 Kerne+
- GPU: Optional (fÃ¼r Ollama)
- Speicher: 10 GB+ (fÃ¼r ChromaDB)

---

## ğŸš¨ Fehlerbehebung

### Ollama Modelle nicht gefunden
```bash
ollama pull qwen2.5:7b
ollama list  # Zeigt installierte Modelle
```

### ChromaDB Fehler
```bash
# Datenbank zurÃ¼cksetzen (Vorsicht! LÃ¶scht alle Erinnerungen)
rm -rf data/chroma_db
```

### Zwei-Schritte System Probleme
```bash
# TemporÃ¤r deaktivieren
/twostep  # Im Chat eingeben
```

### Debug Informationen
```bash
# Debug Log anzeigen
/step1  # Zeigt letzten Step 1 JSON
/debug  # Toggle Debug Mode
```

---

## ğŸ§¹ Projekt aufraeumen

### Cleanup-Script

Mit dem Cleanup-Script kannst du Cache-Dateien und unnoetige Ordner loeschen:

```bash
# Zeigt was geloescht wuerde (ohne zu loeschen)
python cleanup.py --dry-run

# Fuehrt Cleanup durch
python cleanup.py

# Loescht auch ChromaDB (VORSICHT: Alle Erinnerungen!)
python cleanup.py --include-chromadb

# Ohne Bestaetigung
python cleanup.py --yes
```

### Wo liegen die grossen Dateien?

| Speicherort | Beschreibung | Groesse | Loeschen mit |
|-------------|--------------|---------|--------------|
| `venv/` | Virtual Environment | 500 MB - 1 GB | `cleanup.py` |
| `__pycache__/` | Python Bytecode Cache | 10-50 MB | `cleanup.py` |
| `data/chroma_db/` | Langzeitgedaechtnis | Variabel | `cleanup.py --include-chromadb` |
| `~/.cache/huggingface/` | Embedding-Modelle | 500 MB - 2 GB | **Nicht empfohlen** |

### Virtual Environment neu erstellen

Nach dem Cleanup:
```bash
# Windows
python -m venv venv
.\venv\Scripts\activate
pip install -r requirements.txt

# Linux/Mac
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Embedding-Modelle

Die Embedding-Modelle (Sentence Transformers) werden beim ersten Start automatisch heruntergeladen und liegen ausserhalb des Projektordners:
- **Windows:** `%USERPROFILE%\.cache\huggingface\`
- **Linux/Mac:** `~/.cache/huggingface/`

Diese werden **NICHT** vom Cleanup-Script geloescht, da sie sonst bei jedem Start erneut heruntergeladen werden muessten (~500 MB - 2 GB).

---

## ğŸ“ Changelog

### Version 2.0 Update (Februar 2026)
- âœ… **NEUE MODELLE fÃ¼r Step 1:**
  - Cerebras: qwen-3-235b-a22b-instruct-2507
  - Groq: openai/gpt-oss-120b
  - Ollama: gpt-oss-20b (lokal)
- âœ… **Context UI verbessert:** Direkte Anzeige von Soul, User, Prefs
- âœ… **ChromaDB Health Check:** Detaillierte Fehleranzeige statt generischer Warnung
- âœ… **Intent Processor optimiert:** Bessere Tool Call Verarbeitung

### Version 2.0 (2026)
- âœ… **Zwei-Schritte Verarbeitung:** Intent Processor + Response Generator
- âœ… **Neue Context-Dateien:** soul.md, user.md, CHAPPiEsPreferences.md
- âœ… **Short-Term Memory V2:** JSON-basiert mit Timestamps
- âœ… **Smart Tool System:** Automatische Entscheidungen basierend auf Intent
- âœ… **Debug Logger:** Zentrales Logging fÃ¼r CLI und Web UI
- âœ… **ChromaDB Kompatibel:** 100% backward compatible
- âœ… **Training optimiert:** Chappie 200 Tokens (60% reduziert)

---

## ğŸ¤ Contributing

Pull Requests sind willkommen! Bitte:
1. Forke das Repository
2. Erstelle einen Feature Branch
3. Committe deine Ã„nderungen
4. Push zum Branch
5. Ã–ffne einen Pull Request

---

<div align="center">

**[â¬† Nach oben](#-chappie-20---cognitive-hybrid-assistant)**

Made with â¤ï¸ by [017pixel](https://github.com/017pixel)

</div>
